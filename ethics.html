<html>
	<head><meta charset="utf-8">
		<title>Autodrive</title>
		<!-- Meta -->
		<meta content="utf-8" http-equiv="encoding">
		<meta name="viewport" content="width=450">
		<link rel="icon" type="image/png" href="favicon.jpg"/>
    <!-- Styling -->
		<link href="css/index.css?version=1.0.1" rel="stylesheet" />
	</head>
<body>
	<div id="header">
			<div id="nav">
				<span id="nav_names">Autodrive</span>
				<span id="nav_buttons">
					<a class="navlink" href="https://dynamicdesignlab.sites.stanford.edu/"> Visit the Stanford Dynamic Design Lab</a>
				</span>
			</div>
		</div>
		<div id="content">
			&emsp; Autonomous cars will certainly be used by many people very soon because approximately 1.35 million people around the world die each year in traffic accidents and the aspirational goal that autonomous vehicles aim to achieve: to reduce the number of traffic accidents produced by human error<a href="#ref1">[1]</a>. <br> <br>
			<blockquote> <i> However, is it ethical to produce self-driving cars whose choices will affect the lives of the driver and surroundings? What ethical considerations should be taken into account when implementing decision-making algorithms and what ethical framework would be the best for self-driving cars? </i> </blockquote> </br>
			&emsp; One of the main challenges for autonomous vehicles (AVs) is centered around how they value human life. The questions that concern the public and engineers are whether autonomous vehicles are smart enough to calculate the value of one human life over another and how those decision calculations will be made. If a self-driving car makes a mistake, it could directly lead to the loss of life. Consider the famous Trolley Problem, where someone is put in two different situations that present nominally similar choices and potential consequences. The dilemma is: <br> <br>
			<blockquote> <i> Is it morally permissible to kill five people and save one life or vice versa? </i> </blockquote> </br>
			&emsp; In the first situation, a runaway trolley will run into and kill five workmen, saving the life of one workman by flipping a switch and diverting the trains down a sidetrack. In the second situation, the observer has to decide whether he wants to kill five people or push a rather large and plump individual off off the bridge onto the tracks below, thereby stopping the train and saving the five lives. <br> <br>
			&emsp; By adopting a theoretical problem into a real-life situation, it is possible to consider multiple options regarding the moral frameworks and analyze how decisions are made by a self-driving car. Let’s consider a situation where an autonomous car needs to decide whether to kill its owner by hitting a tree or a family of five people crossing the road. From Kant’s principle, a refusal to sacrifice the one to save five would be an ethical stand supported by the belief that individuals are ends in themselves and not means to the end of others. Therefore, a self-driving car that is programmed with Kant’s principle, would always choose inaction over action. However, utilitarians would disagree with the decision made by this car. From a utilitarian stand, the car would always choose the maximax strategy, which saves the option with maximum payoffs. <br> <br>
			&emsp; Driverless cars will not often face life-or-death situations, but they will be constantly making decisions that will still redistribute hazard from one group of stakeholders to another group of stakeholders. For example, from cyclists to drivers or vice-versa. Imagine a situation in which an autonomous vehicle is deciding where to position itself on a highway - closer to a truck to its left, or a bicycle lane on its right. Theoretically and philosophically thinking, the car has to make an ethical decision that would minimize risks. If AVs were always programmed to be slightly closer to the bicycle lane using, they may reduce the likelihood of hitting other cars, while slightly increasing the likelihood of hitting cyclists <a href="#ref2">[2]</a>. Therefore, from a utilitarian perspective, the car would minimize the number of deaths and increase the number of people saved because it would try to avoid colliding with bigger trucks. On the other hand, from the perspective of Rawls, an American moral and political philosopher, under the veil of ignorance it is not morally permissible to be slightly closer to a bicyclist. The veil of ignorance puts an agent into a state of ignorance about her own position in the society <a href="#ref3">[3]</a>. Consider, for instance, that you are behind a veil of ignorance and you do not know whether you will be privileged enough to own a self-driving car or have to use a bicycle as a means of transport. <br> <br>
			<blockquote> <i> Would you then prefer a world where laws prevent drivers from getting too close to cyclists or a world where there are no such rules that protect cyclists?  </i> </blockquote> </br>
			&emsp; In this scenario, according to the difference principle, the inequalities should make the least advantaged (a cyclist) in society materially better off than they would be under strict equality <a href="#ref4">[4]</a>. In Spain, safety laws already require motorists to leave a gap of 1.5 metres when trying to pass bikers on the road, even if this means crossing the unbroken white line in the middle of the road <a href="#ref5">[5]</a>. Even in the self-driverless world, drivers should not be treated differently based on their transport and the difference principle could be used to minimize the chance of hitting cyclists. Therefore, by implementing algorithms that would keep a self-driving car equally away from a bicycle and a truck, we can avoid utilitarian dilemma. <br> <br>
			&emsp; No matter how safe self-driving cars are, we cannot completely reduce the possibility of someone dying on a road. The fact that the general public doesn't trust the concept of autonomous vehicles should be eliminated by “teaching” self-driving cars how to make better decisions based on ethical frameworks. <br> <br>
		<div class="section"> REFERENCES </div>
		<div id="ref1"> </div>
		[1] - World Health Organization (WHO). Global Status Report on Road Safety 2018. December 2018. [cited 2020 October 28]. Available from URL: <a href="https://www.who.int/violence_injury_prevention/road_safety_status/2018/en/externalicon"> https://www.who.int/violence_injury_prevention/road_safety_status/2018/en/externalicon </a> <br> <br>
		<div id="ref2"> </div>
		[2] - Choi, C. (2018). The Moral Dilemmas of Self-Driving Cars. Retrieved from <a href="https://www.insidescience.org/news/moral-dilemmas-self-driving-cars"> https://www.insidescience.org/news/moral-dilemmas-self-driving-cars </a> <br> <br>
		<div id="ref3"> </div>
		[3] - Leben, D. (2017). A Rawlsian algorithm for autonomous vehicles. Ethics and Information Technology. 19 (2):107-115. DOI: 10.1007/s10676-017-9419-3 <br> <br>
		<div id="ref4"> </div>
		[4] - Lamont, J., Christi F. (1996). Distributive Justice.  The Stanford Encyclopedia of Philosophy (Winter 2017 Edition). Retrieved from <a href="https://plato.stanford.edu/archives/win2017/entries/justice-distributive/"> https://plato.stanford.edu/archives/win2017/entries/justice-distributive/ </a> <br> <br>
		<div id="ref5"> </div>
		[5] - Middleton. J. (2021, January 27). Spain ‘to make it illegal’ for drivers not to slow down when overtaking cyclists. The Independent. Retrieved from <a href="https://www.independent.co.uk/news/world/europe/spain-cycling-drivers-overtaking-b1793407.html"> https://www.independent.co.uk/news/world/europe/spain-cycling-drivers-overtaking-b1793407.html </a> <br> <br>
		</div>
		<div id="footer">
			<div id="footer_banner">
				<div>Contact us: <a href="mailto:robert.dumitru@minerva.kgi.edu"> robert.dumitru@minerva.kgi.edu </a> <br>
					<span style="color:#555;">this site is <a href="https://github.com/robert-dumitru/autodrive" style="color:#888">open source</a>, zero rights reserved.</span></div>
				</div>
			</div>
</html>
